# -*- coding: utf-8 -*-
"""roaddl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C4kJTIg6IrV0je_FfUlqxYPuZHD4fW-N
"""

import os
from os.path import join as pjoin
import collections
import json
import torch
import numpy as np
import scipy.misc as m
import scipy.io as io
import matplotlib.pyplot as plt
import glob

from PIL import Image
from tqdm import tqdm
from torch.utils import data
from ptsemseg.utils import recursive_glob
from torch.utils import data
from torchvision import transforms

class roadLoader(data.Dataset):
  """
  Source: https://competitions.codalab.org/competitions/18467#participate-get_starting_kit
  """
  def __init__(
        self,
        root,
        split="training",
        img_size=(1024, 1024),
        is_transform=False,
        augmentations=None,
        img_norm = False,
        test_mode=False,
    ):
    
    self.root = root
    self.split = split
    self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)
    self.is_transform = is_transform
    self.augmentations = augmentations
    self.img_norm = img_norm
    self.test_mode = test_mode
    self.mean = np.array([104.00699, 116.66877, 122.67892])
    self.n_classes = 3
    self.files = collections.defaultdict(list)

     #?what is happening here eek
    if not self.test_mode:
        for split in ["train", "test", "val"]:
            file_list = os.listdir(root + "/" + split)
            self.files[split] = file_list

def __len__(self):
        return len(self.files[self.split])

def __getitem__(self, index):
        """__getitem__
        :param index:
        """
        img_path = self.files[self.split][index].rstrip()
        lbl_path = os.path.join(
            self.annotations_base,
            img_path.split(os.sep)[-2],
            os.path.basename(img_path)[:-15] + "gtFine_labelIds.png",
        )

        img = m.imread(img_path)
        img = np.array(img, dtype=np.uint8)

        lbl = m.imread(lbl_path)
        lbl = self.encode_segmap(np.array(lbl, dtype=np.uint8))

        if self.augmentations is not None:
            img, lbl = self.augmentations(img, lbl)

        if self.is_transform:
            img, lbl = self.transform(img, lbl)

        return img, lbl

def transform(self, img, lbl):
        img = m.imresize(img, (self.img_size[0], self.img_size[1]))  # uint8 with RGB mode
        img = img[:, :, ::-1]  # RGB -> BGR
        img = img.astype(np.float64)
        img -= self.mean
        if self.img_norm:
            # Resize scales images from 0 to 255, thus we need
            # to divide by 255.0
            img = img.astype(float) / 255.0
        # NHWC -> NCHW
        img = img.transpose(2, 0, 1)

        img = torch.from_numpy(img).float()
        lbl = torch.from_numpy(lbl).long()
        return img, lbl

def decode_segmap(self, temp, plot=False):
        
        Road = [0, 168, 107]
        NotRoad = [124, 10, 2]
        Unlabelled = [0, 0, 0]

        label_colours = np.array(
            [
                Road,
                NotRoad,
                Unlabelled
            ]
        )
        r = temp.copy()
        g = temp.copy()
        b = temp.copy()
        for l in range(0, self.n_classes):
            r[temp == l] = label_colours[l, 0]
            g[temp == l] = label_colours[l, 1]
            b[temp == l] = label_colours[l, 2]

        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))
        rgb[:, :, 0] = r / 255.0
        rgb[:, :, 1] = g / 255.0
        rgb[:, :, 2] = b / 255.0
        return rgb

#     def encode_segmap(self, mask):
#         # Refer : http://groups.csail.mit.edu/vision/datasets/ADE20K/code/loadAde20K.m
#         mask = mask.astype(int)
#         label_mask = np.zeros((mask.shape[0], mask.shape[1]))
#         label_mask = (mask[:, :, 0] / 10.0) * 256 + mask[:, :, 1]
#         return np.array(label_mask, dtype=np.uint8)

if __name__ == "__main__":
    local_path = "/Users/Jeba/Downloads/road"
    dst = ADE20KLoader(local_path, is_transform=True)
    trainloader = data.DataLoader(dst, batch_size=16)
    for i, data_samples in enumerate(trainloader):
        imgs, labels = data_samples
        if i == 0:
            img = torchvision.utils.make_grid(imgs).numpy()
            img = np.transpose(img, (1, 2, 0))
            img = img[:, :, ::-1]
            plt.imshow(img)
            plt.show()
            for j in range(4):
                plt.imshow(dst.decode_segmap(labels.numpy()[j]))
                plt.show()